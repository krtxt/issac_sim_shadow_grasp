"""
æ‰¹é‡Isaac Gymç¨³å®šæ€§æµ‹è¯•è°ƒåº¦è„šæœ¬

èŒè´£ï¼šè¯»å–DexGraspNetæ•°æ®ã€æ‹†åˆ†ä»»åŠ¡ã€ç”Ÿæˆtask_specã€GPUè°ƒåº¦ã€è¿›ç¨‹ç›‘æ§ã€æ¸…ç†å¤±è´¥ä»»åŠ¡
"""
import os
import sys
import subprocess
import random
import time
import shutil
import json
from datetime import datetime
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any
from dataclasses import dataclass, field
from collections import OrderedDict
from enum import Enum
import tyro
import logging
import signal
import psutil
from rich import box
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.rot6d import rot_to_orthod6d

# é»˜è®¤å…ƒæ•°æ®æ–‡ä»¶
DEFAULT_METADATA_FILES = [
    # 'debug_grasp_data_256.pt',
    # 'debug_grasp_data_128.pt',
    'dexgraspnet_shadowhand_downsample.pt',
    # 'dexgraspnet_shadowhand.pt',
]


console = Console()


@dataclass
class Config:
    """ç¨³å®šæ€§è¯„ä¼°çš„é…ç½®å‚æ•°"""
    # æ•°æ®è·¯å¾„
    dataset_path: str = '/home/xiantuo/source/grasp/GithubClone/SceneLeapUltra/data/DexGraspNet'
    object_root: Optional[str] = None  # ç‰©ä½“urdf/objæ ¹ç›®å½•ï¼ŒNoneåˆ™ä½¿ç”¨dataset_path/meshdata
    metadata_file: Optional[str] = None  # å¯é€‰ï¼Œè¦†ç›–é»˜è®¤metadataæ–‡ä»¶
    
    # è¾“å‡ºè·¯å¾„
    output_root: str = 'outputs/stability_eval/test_record_all'
    # task_spec_root: str = 'outputs/stability_eval/task_specs'
    task_spec_root: str = os.path.join(output_root, 'task_specs')
    # log_dir: str = 'outputs/stability_eval/logs'
    log_dir: str = os.path.join(output_root, 'logs')
    
    # æ•°æ®/ä»»åŠ¡é…ç½®
    max_objects: int = -1 
    max_grasps_per_object: int = -1 
    split: str = 'all'  # 'train', 'test', or 'all'
    
    # è°ƒåº¦é…ç½®
    # gpu_ids: List[int] = field(default_factory=lambda: [0])
    gpu_ids: List[int] = field(default_factory=lambda: [0,1,2,4,5,6,7])
    jobs_per_gpu: int = 1
    timeout_duration: int = 600  # æ¯ä¸ªä»»åŠ¡è¶…æ—¶æ—¶é—´(ç§’)
    
    # å½•åˆ¶é€‰é¡¹
    enable_recording: bool = False
    save_hand_only_video: bool = False
    save_visualization: bool = False
    
    # è¡Œä¸º
    debug: bool = True
    onscreen: bool = False
    cpu: bool = False
    static_preview: bool = False
    
    def __post_init__(self):
        """åˆå§‹åŒ–è·¯å¾„"""
        if self.object_root is None:
            self.object_root = os.path.join(self.dataset_path, 'meshdata')
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        os.makedirs(self.output_root, exist_ok=True)
        os.makedirs(self.task_spec_root, exist_ok=True)
        os.makedirs(self.log_dir, exist_ok=True)
        
        # éªŒè¯å‚æ•°
        if not self.gpu_ids:
            raise ValueError("gpu_idsä¸èƒ½ä¸ºç©º")
        if self.jobs_per_gpu < 1:
            raise ValueError("jobs_per_gpuå¿…é¡»å¤§äº0")
        if self.timeout_duration < 1:
            raise ValueError("timeout_durationå¿…é¡»å¤§äº0")


class TaskStatus(Enum):
    """ä»»åŠ¡çŠ¶æ€"""
    SUCCESS = "success"
    TIMEOUT = "timeout"
    ERROR = "error"


def render_intro(config: "Config"):
    """ä½¿ç”¨ Rich å±•ç¤ºä»»åŠ¡é…ç½®æ¦‚è§ˆ"""
    console.rule("[bold cyan]Isaac Gym ç¨³å®šæ€§è¯„ä¼°æ‰¹é‡è°ƒåº¦")
    info_table = Table(show_header=False, box=box.SIMPLE_HEAVY, expand=True)
    info_table.add_row("æ•°æ®é›†è·¯å¾„", config.dataset_path)
    info_table.add_row("è¾“å‡ºè·¯å¾„", config.output_root)
    info_table.add_row(
        "GPU è°ƒåº¦",
        f"IDs: {config.gpu_ids} / æ¯GPUä»»åŠ¡æ•°: {config.jobs_per_gpu}",
    )
    info_table.add_row(
        "ç‰©ä½“/æŠ“å–ä¸Šé™",
        f"ç‰©ä½“ {config.max_objects} Â· æ¯ç‰©ä½“æŠ“å– {config.max_grasps_per_object}",
    )
    info_table.add_row(
        "å½•åˆ¶",
        f"è§†é¢‘:{'å¼€' if config.enable_recording else 'å…³'} Â· æ‰‹éƒ¨:{'å¼€' if config.save_hand_only_video else 'å…³'} Â· å¯è§†åŒ–:{'å¼€' if config.save_visualization else 'å…³'}",
    )
    console.print(info_table)
    console.rule()


def render_progress_panel(
    successful: int,
    total: int,
    attempt: int,
    timeout: Optional[int] = None,
    error: Optional[int] = None,
    title: str = "è¿›åº¦æ›´æ–°",
    style: str = "cyan",
):
    """æ¸²æŸ“å¸¦ç»Ÿè®¡ä¿¡æ¯çš„è¿›åº¦é¢æ¿"""
    table = Table(show_header=False, box=box.SIMPLE, expand=False)
    table.add_row("æˆåŠŸä»»åŠ¡", f"{successful}/{total}")
    table.add_row("æ€»å°è¯•", str(attempt))
    if timeout is not None:
        table.add_row("è¶…æ—¶ä»»åŠ¡", str(timeout))
    if error is not None:
        table.add_row("é”™è¯¯ä»»åŠ¡", str(error))
    console.print(Panel.fit(table, title=title, border_style=style))


def render_final_summary(total: int, attempt: int, stats: dict):
    """æ¸²æŸ“æœ€ç»ˆç»Ÿè®¡é¢æ¿"""
    success = stats.get("successful_tasks", 0)
    timeout = stats.get("timeout_tasks", 0)
    error = stats.get("error_tasks", 0)
    if error > 0:
        border = "red"
    elif timeout > 0:
        border = "yellow"
    else:
        border = "green"

    summary_table = Table(show_header=False, box=box.SIMPLE_HEAVY, expand=False)
    summary_table.add_row("æ€»ä»»åŠ¡", str(total))
    summary_table.add_row("æˆåŠŸ", str(success))
    summary_table.add_row("è¶…æ—¶", str(timeout))
    summary_table.add_row("é”™è¯¯", str(error))
    summary_table.add_row("æ€»å°è¯•", str(attempt))

    console.print(Panel(summary_table, title="è¯„ä¼°å®Œæˆç»Ÿè®¡", border_style=border))


class TaskLogger:
    """ä»»åŠ¡æ—¥å¿—è®°å½•å™¨"""
    def __init__(self, log_dir: str):
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.log_dir = log_dir

        self.logger = logging.getLogger("StabilityEval")
        self.logger.setLevel(logging.INFO)
        self.logger.propagate = False
        self.logger.handlers.clear()

        # è¯¦ç»†æ—¥å¿—æ–‡ä»¶
        log_file = os.path.join(log_dir, f"stability_eval_{timestamp}.log")
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s'
        ))
        self.logger.addHandler(file_handler)

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats_file = os.path.join(log_dir, f"task_stats_{timestamp}.json")
        self.stats = {
            "total_tasks": 0,
            "successful_tasks": 0,
            "timeout_tasks": 0,
            "error_tasks": 0,
            "task_details": []
        }
        
        self.successful_task_ids = set()
        self.console = console

    def _print_task_start(self, task_id: str, gpu_id: int):
        message = f"åˆ†é… GPU {gpu_id}"
        panel_title = f"ğŸš€ å¯åŠ¨ {task_id}"
        self.console.print(Panel.fit(message, title=panel_title, border_style="cyan"))

    def _print_task_result(
        self,
        status: TaskStatus,
        task_id: str,
        duration: float,
        folder: str,
        gpu_id: int,
        error_msg: Optional[str] = None,
    ):
        style_map = {
            TaskStatus.SUCCESS: "green",
            TaskStatus.TIMEOUT: "yellow",
            TaskStatus.ERROR: "red",
        }
        icon_map = {
            TaskStatus.SUCCESS: "âœ…",
            TaskStatus.TIMEOUT: "â°",
            TaskStatus.ERROR: "âŒ",
        }
        title_map = {
            TaskStatus.SUCCESS: "ä»»åŠ¡æˆåŠŸ",
            TaskStatus.TIMEOUT: "ä»»åŠ¡è¶…æ—¶",
            TaskStatus.ERROR: "ä»»åŠ¡å¤±è´¥",
        }

        details = [f"è€—æ—¶ {duration:.2f}s", f"GPU {gpu_id}", f"ç›®å½• {folder}"]
        if error_msg:
            details.append(f"åŸå› : {error_msg}")
        detail_text = "\n".join(details)
        panel_message = f"{icon_map[status]} [bold]{task_id}[/bold]\n{detail_text}"
        self.console.print(
            Panel.fit(panel_message, title=title_map[status], border_style=style_map[status])
        )

    def log_task_start(self, task_id: str, gpu_id: int):
        """è®°å½•ä»»åŠ¡å¼€å§‹"""
        self.logger.info(f"[{task_id}] å¼€å§‹ä»»åŠ¡ on GPU {gpu_id}")
        self.stats["total_tasks"] += 1
        self._print_task_start(task_id, gpu_id)

    def log_task_end(self, task_id: str, status: TaskStatus, duration: float,
                     folder: str, gpu_id: int, error_msg: Optional[str] = None):
        """è®°å½•ä»»åŠ¡ç»“æŸ"""
        if status == TaskStatus.SUCCESS:
            self.logger.info(f"[{task_id}] ä»»åŠ¡æˆåŠŸå®Œæˆ in {duration:.2f}s")
            if task_id not in self.successful_task_ids:
                self.stats["successful_tasks"] += 1
                self.successful_task_ids.add(task_id)
        elif status == TaskStatus.TIMEOUT:
            self.logger.warning(f"[{task_id}] ä»»åŠ¡è¶…æ—¶ after {duration:.2f}s")
            self.stats["timeout_tasks"] += 1
        else:
            error_details = f": {error_msg}" if error_msg else ""
            self.logger.error(f"[{task_id}] ä»»åŠ¡å¤±è´¥ after {duration:.2f}s{error_details}")
            self.stats["error_tasks"] += 1

        task_detail = {
            "task_id": task_id,
            "status": status.value,
            "duration": duration,
            "folder": folder,
            "gpu_id": gpu_id,
        }
        if error_msg:
            task_detail["error_message"] = error_msg

        self.stats["task_details"].append(task_detail)
        self._save_stats()
        self._print_task_result(status, task_id, duration, folder, gpu_id, error_msg)
    
    def _save_stats(self):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with open(self.stats_file, 'w') as f:
            json.dump(self.stats, f, indent=2)


class GPUManager:
    """GPUèµ„æºç®¡ç†å™¨"""
    def __init__(self, gpu_ids: List[int], jobs_per_gpu: int):
        self.gpu_ids = gpu_ids
        self.jobs_per_gpu = jobs_per_gpu
        self.gpu_job_counts = {gpu_id: 0 for gpu_id in gpu_ids}
    
    def get_available_gpu(self) -> int:
        """è¿”å›å½“å‰è´Ÿè½½æœ€å°çš„GPU ID"""
        return min(self.gpu_job_counts.items(), key=lambda x: x[1])[0]
    
    def add_job(self, gpu_id: int):
        """ä¸ºæŒ‡å®šGPUæ·»åŠ ä¸€ä¸ªä»»åŠ¡"""
        self.gpu_job_counts[gpu_id] += 1
    
    def remove_job(self, gpu_id: int):
        """ä¸ºæŒ‡å®šGPUç§»é™¤ä¸€ä¸ªä»»åŠ¡"""
        self.gpu_job_counts[gpu_id] = max(0, self.gpu_job_counts[gpu_id] - 1)
    
    @property
    def total_jobs(self) -> int:
        """è¿”å›å½“å‰æ€»ä»»åŠ¡æ•°"""
        return sum(self.gpu_job_counts.values())
    
    @property
    def max_parallel_jobs(self) -> int:
        """è¿”å›æœ€å¤§å¹¶è¡Œä»»åŠ¡æ•°"""
        return len(self.gpu_ids) * self.jobs_per_gpu


class ProcessManager:
    """è¿›ç¨‹ç®¡ç†å™¨"""
    def __init__(self):
        self.active_processes = []
        signal.signal(signal.SIGINT, self.handle_interrupt)
        signal.signal(signal.SIGTERM, self.handle_interrupt)
    
    def add_process(self, process, start_time, gpu_id, task_id, folder):
        """æ·»åŠ æ–°è¿›ç¨‹"""
        self.active_processes.append((process, start_time, gpu_id, task_id, folder))
    
    def update_processes_status(
        self, 
        gpu_manager: GPUManager, 
        task_logger: TaskLogger, 
        config: Config, 
        processed_task_ids: set
    ) -> int:
        """æ£€æŸ¥æ‰€æœ‰æ´»åŠ¨è¿›ç¨‹çš„çŠ¶æ€"""
        new_active_processes = []
        newly_successful_tasks = 0
        current_time = time.time()

        for proc, start_time, gpu_id, task_id, folder in self.active_processes:
            elapsed_time = current_time - start_time
            folder_path = os.path.join(config.output_root, folder)

            try:
                # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜åœ¨
                if not psutil.pid_exists(proc.pid):
                    gpu_manager.remove_job(gpu_id)
                    cleanup_task_folder(folder_path)
                    task_logger.log_task_end(task_id, TaskStatus.ERROR, elapsed_time, folder, gpu_id, 
                                           "è¿›ç¨‹æ„å¤–ç»ˆæ­¢")
                    continue

                # æ£€æŸ¥è¶…æ—¶
                if elapsed_time > config.timeout_duration:
                    parent = psutil.Process(proc.pid)
                    children = parent.children(recursive=True)
                    for child in children:
                        child.kill()
                    parent.kill()
                    
                    gpu_manager.remove_job(gpu_id)
                    cleanup_task_folder(folder_path)
                    task_logger.log_task_end(task_id, TaskStatus.TIMEOUT, elapsed_time, folder, gpu_id)
                    continue

                # æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
                if proc.poll() is not None:
                    gpu_manager.remove_job(gpu_id)
                    
                    if task_id not in processed_task_ids:
                        processed_task_ids.add(task_id)
                        
                        if proc.returncode == 0:
                            # æ­£å¸¸é€€å‡ºï¼Œç›´æ¥è§†ä¸ºæˆåŠŸ
                            task_logger.log_task_end(task_id, TaskStatus.SUCCESS, elapsed_time, folder, gpu_id)
                            newly_successful_tasks += 1
                        else:
                            # éé›¶é€€å‡ºç ï¼šå…ˆæ£€æŸ¥æ˜¯å¦å·²æˆåŠŸäº§å‡º metrics.json
                            metrics_path = os.path.join(folder_path, "metrics.json")
                            has_valid_metrics = False
                            if os.path.exists(metrics_path):
                                try:
                                    with open(metrics_path, "r") as f:
                                        json.load(f)
                                    has_valid_metrics = True
                                except Exception:
                                    has_valid_metrics = False

                            if has_valid_metrics:
                                # ä»¿çœŸæµç¨‹å·²å®Œæˆä¸”ç»“æœæ–‡ä»¶å­˜åœ¨ï¼Œåªæ˜¯åœ¨é€€å‡ºé˜¶æ®µå‘ç”Ÿæ®µé”™è¯¯ç­‰ï¼Œè§†ä¸ºæˆåŠŸ
                                task_logger.log_task_end(task_id, TaskStatus.SUCCESS, elapsed_time, folder, gpu_id)
                                newly_successful_tasks += 1
                            else:
                                # æ²¡æœ‰æœ‰æ•ˆç»“æœæ–‡ä»¶ï¼Œæ‰è§†ä¸ºçœŸæ­£å¤±è´¥å¹¶æ¸…ç†ç›®å½•
                                cleanup_task_folder(folder_path)
                                task_logger.log_task_end(
                                    task_id,
                                    TaskStatus.ERROR,
                                    elapsed_time,
                                    folder,
                                    gpu_id,
                                    f"é€€å‡ºç : {proc.returncode}",
                                )
                else:
                    new_active_processes.append((proc, start_time, gpu_id, task_id, folder))
                    
            except psutil.NoSuchProcess:
                gpu_manager.remove_job(gpu_id)
                cleanup_task_folder(folder_path)
                task_logger.log_task_end(task_id, TaskStatus.ERROR, elapsed_time, folder, gpu_id,
                                       "è¿›ç¨‹æ¶ˆå¤±")

        self.active_processes = new_active_processes
        return newly_successful_tasks

    def handle_interrupt(self, signum, frame):
        """å¤„ç†ä¸­æ–­ä¿¡å·"""
        console.print("\n[bold yellow]æ­£åœ¨æ¸…ç†è¿›ç¨‹...[/]")
        try:
            self.cleanup_all_processes()
        except Exception as e:
            console.print(f"[bold red]æ¸…ç†è¿›ç¨‹æ—¶å‘ç”Ÿé”™è¯¯:[/] {e}")
        finally:
            console.print("[bold green]æ¸…ç†å®Œæˆ[/]")
            exit(0)
    
    def cleanup_all_processes(self):
        """æ¸…ç†æ‰€æœ‰è¿›ç¨‹"""
        for proc, _, gpu_id, task_id, folder in self.active_processes:
            try:
                if psutil.pid_exists(proc.pid):
                    parent = psutil.Process(proc.pid)
                    timeout = 5
                    start_time = time.time()
                    
                    parent.terminate()
                    
                    while time.time() - start_time < timeout:
                        if not parent.is_running():
                            break
                        time.sleep(0.1)
                    
                    if parent.is_running():
                        children = parent.children(recursive=True)
                        for child in children:
                            child.kill()
                        parent.kill()
                    
                    console.print(f"[yellow]å·²ç»ˆæ­¢ä»»åŠ¡ {task_id}[/]")
            except psutil.NoSuchProcess:
                pass
            except Exception as e:
                console.print(f"[bold red]ç»ˆæ­¢ä»»åŠ¡ {task_id} æ—¶å‡ºé”™:[/] {e}")


def generate_unique_id():
    """ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦"""
    return random.randint(0, 2**32-1)


def cleanup_task_folder(folder_path: str):
    """æ¸…ç†å¤±è´¥çš„ä»»åŠ¡æ–‡ä»¶å¤¹"""
    try:
        if os.path.exists(folder_path):
            for root, dirs, files in os.walk(folder_path, topdown=False):
                for name in files:
                    try:
                        os.remove(os.path.join(root, name))
                    except Exception as e:
                        logging.warning(f"æ— æ³•åˆ é™¤æ–‡ä»¶ {name}: {str(e)}")
                for name in dirs:
                    try:
                        os.rmdir(os.path.join(root, name))
                    except Exception as e:
                        logging.warning(f"æ— æ³•åˆ é™¤ç›®å½• {name}: {str(e)}")
            shutil.rmtree(folder_path)
            logging.info(f"æˆåŠŸæ¸…ç†æ–‡ä»¶å¤¹: {folder_path}")
    except Exception as e:
        logging.error(f"æ¸…ç†æ–‡ä»¶å¤¹ {folder_path} å¤±è´¥: {str(e)}")


def aggregate_successful_results(config: Config, task_logger: TaskLogger):
    """èšåˆæ‰€æœ‰æˆåŠŸä»»åŠ¡çš„ç»“æœå¹¶æ¸…ç†å•ä»»åŠ¡è¾“å‡ºç›®å½•"""
    successful_ids = sorted(task_logger.successful_task_ids)
    if not successful_ids:
        print("æ²¡æœ‰æˆåŠŸå®Œæˆçš„ä»»åŠ¡ï¼Œæ— éœ€æ±‡æ€»ã€‚")
        return

    html_dir = os.path.join(config.output_root, "html")
    video_dir = os.path.join(config.output_root, "videos")
    hand_video_dir = os.path.join(config.output_root, "hand_only_videos")
    metrics_summary_path = os.path.join(config.output_root, "metrics_summary.json")

    output_dirs = [html_dir, video_dir]
    if config.save_hand_only_video:
        output_dirs.append(hand_video_dir)

    for path in output_dirs:
        if os.path.isdir(path):
            shutil.rmtree(path)
        os.makedirs(path, exist_ok=True)

    aggregated_metrics: List[Dict[str, Any]] = []
    processed_task_dirs = []

    for task_id in successful_ids:
        folder_name = f"task_{task_id}"
        folder_path = os.path.join(config.output_root, folder_name)
        if not os.path.isdir(folder_path):
            continue

        metrics_path = os.path.join(folder_path, "metrics.json")
        if os.path.exists(metrics_path):
            try:
                with open(metrics_path, "r") as f:
                    metrics = json.load(f)
                aggregated_metrics.append(metrics)
            except Exception as exc:
                logging.error(f"è¯»å– {metrics_path} å¤±è´¥: {exc}")

        for root, _, files in os.walk(folder_path):
            for filename in files:
                src = os.path.join(root, filename)
                lower = filename.lower()
                dest_name = f"{task_id}_{filename}"

                try:
                    if lower.endswith(".html"):
                        shutil.copy2(src, os.path.join(html_dir, dest_name))
                    elif lower.endswith(".mp4"):
                        if "hand_only" in lower:
                            if config.save_hand_only_video:
                                shutil.copy2(src, os.path.join(hand_video_dir, dest_name))
                        else:
                            shutil.copy2(src, os.path.join(video_dir, dest_name))
                except Exception as exc:
                    logging.error(f"å¤åˆ¶æ–‡ä»¶ {src} å¤±è´¥: {exc}")

        processed_task_dirs.append(folder_path)

    if aggregated_metrics:
        try:
            with open(metrics_summary_path, "w") as f:
                json.dump(aggregated_metrics, f, indent=2, ensure_ascii=False)
            print(f"å·²ç”ŸæˆæŒ‡æ ‡æ±‡æ€»æ–‡ä»¶: {metrics_summary_path}")
        except Exception as exc:
            logging.error(f"å†™å…¥æŒ‡æ ‡æ±‡æ€»å¤±è´¥: {exc}")

    for folder_path in processed_task_dirs:
        try:
            shutil.rmtree(folder_path)
            print(f"å·²æ¸…ç†ä»»åŠ¡ç›®å½•: {folder_path}")
        except Exception as exc:
            logging.error(f"åˆ é™¤ä»»åŠ¡ç›®å½• {folder_path} å¤±è´¥: {exc}")


def load_dexgraspnet_split(dataset_path, split='test'):
    """åŠ è½½DexGraspNetæ•°æ®é›†åˆ†å‰²"""
    split_path = os.path.join(dataset_path, 'grasp.json')
    if not os.path.exists(split_path):
        raise FileNotFoundError(f"ç¼ºå°‘ grasp.json: {split_path}")
    with open(split_path, 'r') as f:
        split_data = json.load(f)
    key_map = {
        'train': '_train_split',
        'test': '_test_split',
        'all': '_all_split'
    }
    split_key = key_map.get(split, split)
    if split_key not in split_data:
        raise KeyError(f"grasp.json ä¸­ç¼ºå°‘ {split_key}")
    return split_data[split_key]


def find_metadata_file(dataset_path, metadata_file=None):
    """æŸ¥æ‰¾metadataæ–‡ä»¶"""
    if metadata_file:
        candidate = Path(metadata_file)
        if not candidate.is_absolute():
            candidate = Path(dataset_path) / candidate
        if candidate.exists():
            return str(candidate.resolve())
        raise FileNotFoundError(f"æŒ‡å®šçš„ metadata æ–‡ä»¶ä¸å­˜åœ¨: {candidate}")

    dataset_dir = Path(dataset_path)
    for filename in DEFAULT_METADATA_FILES:
        candidate = dataset_dir / filename
        if candidate.exists():
            return str(candidate.resolve())
    raise FileNotFoundError(
        f"åœ¨ {dataset_dir} ä¸­æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ metadata æ–‡ä»¶ï¼Œ"
        f"éœ€è¦å…¶ä¸­ä¹‹ä¸€: {DEFAULT_METADATA_FILES}"
    )


def load_dexgraspnet_gt(dataset_path, split='test', metadata_file=None):
    """åŠ è½½DexGraspNet ground truthæ•°æ®"""
    split_objects = load_dexgraspnet_split(dataset_path, split)
    ordered = OrderedDict((obj, []) for obj in split_objects)
    pt_path = find_metadata_file(dataset_path, metadata_file)
    console.log(f"ä½¿ç”¨ DexGraspNet metadata: {pt_path}")
    grasp_dataset = torch.load(pt_path, map_location='cpu')
    metadata = grasp_dataset.get('metadata', [])
    for mdata in metadata:
        obj_name = mdata['object_name']
        if obj_name not in ordered:
            continue
        hand_rot_mat = mdata['rotations'].clone().detach().cpu().float()
        joint = mdata['joint_positions'].clone().detach().cpu().float()
        trans = mdata['translations'].clone().detach().cpu().float()
        scale = float(mdata['scale'])
        rot6d = rot_to_orthod6d(hand_rot_mat.unsqueeze(0)).squeeze(0)
        trans_world = torch.matmul(hand_rot_mat, trans)
        qpos = torch.cat([trans_world, rot6d, joint], dim=0).numpy()
        ordered[obj_name].append({
            'qpos': qpos,
            'scale': scale,
        })
    return ordered


def create_task_spec(task_id, object_name, object_root, qpos_batch, scale_list, config):
    """åˆ›å»ºä»»åŠ¡é…ç½®æ–‡ä»¶"""
    task_spec = {
        'task_id': task_id,
        'object_name': object_name,
        'object_root': object_root,
        'qpos_batch': [qpos.tolist() if hasattr(qpos, 'tolist') else qpos for qpos in qpos_batch],
        'scale_list': scale_list,
        'sim_config': {
            'headless': not config.onscreen,
            'cpu': config.cpu,
        },
        'record_options': {
            'enable_recording': config.enable_recording,
            'save_hand_only': config.save_hand_only_video,
            'save_visualization': config.save_visualization,
        },
        'debug': config.debug,
    }
    
    task_spec_path = os.path.join(config.task_spec_root, f"{task_id}.json")
    with open(task_spec_path, 'w') as f:
        json.dump(task_spec, f, indent=2)
    
    return task_spec_path


def generate_task_command(config: Config, gpu_id: int, task_id: str, 
                          task_spec_path: str, task_output_dir: str) -> Tuple[List[str], dict]:
    """ç”Ÿæˆå•ä¸ªä»»åŠ¡çš„å‘½ä»¤"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    single_script = os.path.join(script_dir, "stability_eval_single.py")
    
    if not os.path.exists(single_script):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°å•ä»¿çœŸè„šæœ¬: {single_script}")
    
    cmd = [
        sys.executable,
        single_script,
        '--task_spec', task_spec_path,
        '--output_dir', task_output_dir,
    ]
    
    if config.onscreen:
        cmd.append('--onscreen')
    if config.cpu:
        cmd.append('--cpu')
    if config.debug:
        cmd.append('--debug')
    if config.static_preview:
        cmd.append('--static_preview')
    
    env = os.environ.copy()
    env["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
    
    return cmd, env


def main(config: Config):
    """ä¸»å‡½æ•°"""
    render_intro(config)
    
    # åˆ›å»ºæ—¥å¿—å’Œç®¡ç†å™¨
    task_logger = TaskLogger(config.log_dir)
    gpu_manager = GPUManager(config.gpu_ids, config.jobs_per_gpu)
    process_manager = ProcessManager()
    
    try:
        # åŠ è½½DexGraspNetæ•°æ®
        console.log("åŠ è½½DexGraspNetæ•°æ®...")
        gt_data = load_dexgraspnet_gt(
            config.dataset_path,
            split=config.split,
            metadata_file=config.metadata_file,
        )
        num_objects_total = len(gt_data)
        num_objects_with_grasps = sum(1 for _name, grasps in gt_data.items() if len(grasps) > 0)
        console.print(
            Panel.fit(
                f"æ€»ç‰©ä½“ {num_objects_total}\nå¯ç”¨ç‰©ä½“ {num_objects_with_grasps}",
                title="DexGraspNet æ•°æ®åŠ è½½å®Œæˆ",
                border_style="green" if num_objects_with_grasps else "red",
            )
        )

        # ä»¥ (object_name, scale) ä½œä¸ºæœ€å°å•å…ƒè¿›è¡Œåˆ†ç»„
        # æ¯ä¸ªåˆ†ç»„å¯¹åº”ä¸€ä¸ªç‰©ä½“åœ¨æŸä¸ªç‰¹å®šscaleä¸‹çš„æ‰€æœ‰æŠ“å–
        grouped_by_obj_scale = {}
        for obj_name, grasps in gt_data.items():
            if len(grasps) == 0:
                continue
            for g in grasps:
                s = float(g["scale"])
                key = (obj_name, s)
                if key not in grouped_by_obj_scale:
                    grouped_by_obj_scale[key] = []
                grouped_by_obj_scale[key].append(g)

        num_groups_total = len(grouped_by_obj_scale)
        console.log(f"æŒ‰ (object_name, scale) åˆ†ç»„å¾—åˆ° {num_groups_total} ä¸ªå€™é€‰ä»»åŠ¡å•å…ƒ")

        # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨ï¼šéå†æ‰€æœ‰ (object_name, scale) åˆ†ç»„ï¼Œç›´åˆ°è¾¾åˆ° max_objects
        max_objects_limit = config.max_objects if config.max_objects > 0 else None
        tasks = []
        task_id_counter = 0
        for (obj_name, scale), grasps in grouped_by_obj_scale.items():
            if len(grasps) == 0:
                continue

            # æˆªæ–­åˆ°æ¯ä¸ª (object, scale) ç»„å†…æœ€å¤šæŠ“å–æ•°
            if config.max_grasps_per_object and config.max_grasps_per_object > 0:
                selected_grasps = grasps[: config.max_grasps_per_object]
            else:
                selected_grasps = grasps
            if len(selected_grasps) == 0:
                continue

            qpos_batch = [g["qpos"] for g in selected_grasps]
            # è¯¥ä»»åŠ¡å†…æ‰€æœ‰æŠ“å–å…±äº«åŒä¸€ä¸ªscale
            scale_list = [float(scale)] * len(selected_grasps)

            task_id = f"{obj_name}_s{scale:.6f}_{task_id_counter:04d}"
            task_id_counter += 1

            tasks.append(
                {
                    "task_id": task_id,
                    "object_name": obj_name,
                    "qpos_batch": qpos_batch,
                    "scale_list": scale_list,
                }
            )

            if max_objects_limit is not None and len(tasks) >= max_objects_limit:
                break

        if len(tasks) == 0:
            console.print("[bold yellow]è­¦å‘Š: æ²¡æœ‰ä»»ä½• (object, scale) ç»„åˆå¯ç”¨äºæµ‹è¯•[/]")
        else:
            console.print(
                Panel.fit(
                    f"å…± {len(tasks)} ä¸ªä»»åŠ¡ (é™åˆ¶ {config.max_objects})",
                    title="ä»»åŠ¡å‡†å¤‡å®Œæˆ",
                    border_style="cyan",
                )
            )
        
        # æ‰§è¡Œä»»åŠ¡è°ƒåº¦
        successful_tasks = 0
        attempt_num = 0
        processed_task_ids = set()
        task_queue = tasks.copy()
        
        while successful_tasks < len(tasks):
            # æ›´æ–°è¿›ç¨‹çŠ¶æ€
            newly_finished_count = process_manager.update_processes_status(
                gpu_manager, task_logger, config, processed_task_ids
            )
            if newly_finished_count > 0:
                successful_tasks += newly_finished_count
                render_progress_panel(
                    successful_tasks,
                    len(tasks),
                    attempt_num,
                    timeout=task_logger.stats["timeout_tasks"],
                    error=task_logger.stats["error_tasks"],
                    title="ä»»åŠ¡è¿›åº¦",
                    style="green",
                )

            # å¯åŠ¨æ–°ä»»åŠ¡
            while gpu_manager.total_jobs < gpu_manager.max_parallel_jobs and task_queue:
                task_info = task_queue.pop(0)
                attempt_num += 1
                
                task_id = task_info['task_id']
                task_folder_name = f"task_{task_id}"
                task_output_dir = os.path.join(config.output_root, task_folder_name)
                
                # åˆ›å»ºtask_specæ–‡ä»¶
                task_spec_path = create_task_spec(
                    task_id,
                    task_info['object_name'],
                    config.object_root,
                    task_info['qpos_batch'],
                    task_info['scale_list'],
                    config
                )
                
                # ç”Ÿæˆå‘½ä»¤
                gpu_id = gpu_manager.get_available_gpu()
                gpu_manager.add_job(gpu_id)
                
                cmd, env = generate_task_command(config, gpu_id, task_id, 
                                                task_spec_path, task_output_dir)
                task_logger.log_task_start(task_id, gpu_id)
                
                # å¯åŠ¨è¿›ç¨‹
                process = subprocess.Popen(cmd, env=env)
                time.sleep(0.5)
                
                if process.poll() is None:
                    os.makedirs(task_output_dir, exist_ok=True)
                    process_manager.add_process(process, time.time(), gpu_id, 
                                              task_id, task_folder_name)
                else:
                    gpu_manager.remove_job(gpu_id)
                    task_logger.log_task_end(task_id, TaskStatus.ERROR, 0.5, 
                                           task_folder_name, gpu_id,
                                           f"è¿›ç¨‹å¯åŠ¨å¤±è´¥: {process.returncode}")
            
            # å®šæœŸä¿å­˜ç»Ÿè®¡
            if attempt_num > 0 and attempt_num % 10 == 0:
                task_logger._save_stats()
                render_progress_panel(
                    successful_tasks,
                    len(tasks),
                    attempt_num,
                    timeout=task_logger.stats["timeout_tasks"],
                    error=task_logger.stats["error_tasks"],
                    title="é˜¶æ®µæ±‡æŠ¥",
                    style="magenta",
                )
            
            # ç­‰å¾…
            if (gpu_manager.total_jobs >= gpu_manager.max_parallel_jobs or
               (successful_tasks + gpu_manager.total_jobs >= len(tasks) and 
                process_manager.active_processes)):
                time.sleep(1)
            elif not process_manager.active_processes and successful_tasks < len(tasks):
                time.sleep(0.1)

        # ç­‰å¾…æ‰€æœ‰å‰©ä½™è¿›ç¨‹å®Œæˆ
        while process_manager.active_processes:
            newly_finished_count = process_manager.update_processes_status(
                gpu_manager, task_logger, config, processed_task_ids
            )
            if newly_finished_count > 0:
                successful_tasks += newly_finished_count
                render_progress_panel(
                    successful_tasks,
                    len(tasks),
                    attempt_num,
                    timeout=task_logger.stats["timeout_tasks"],
                    error=task_logger.stats["error_tasks"],
                    title="æ”¶å°¾ä»»åŠ¡è¿›åº¦",
                    style="blue",
                )
            time.sleep(1)
        
        aggregate_successful_results(config, task_logger)
            
    except KeyboardInterrupt:
        console.print("\n[bold yellow]æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨æ¸…ç†...[/]")
        process_manager.cleanup_all_processes()
    finally:
        task_logger._save_stats()
        render_final_summary(len(tasks), attempt_num, task_logger.stats)


if __name__ == "__main__":
    config = tyro.cli(Config)
    main(config)
